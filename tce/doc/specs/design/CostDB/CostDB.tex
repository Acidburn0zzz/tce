\documentclass[a4paper,twoside]{tce}
\usepackage{pslatex}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

% used inside algorithms to make them consistent
\newcommand{\param}{\emph}
\newcommand{\var}{\emph}
\newcommand{\algoreturn}{\textbf{return} }
\newcommand{\return}{\emph}
\newcommand{\algolinenumbering}{1}

\begin{document}
\author{Tommi Rantanen}
\title{Cost Database}
\ver{0.1.1}
\firstday{14.10.2004}
\lastday{27.04.2006}
% id number in S- sequence
\docnum{034}
% draft/complete/committed
\state{draft}

\maketitle


\chapter*{Document History}

\begin{HistoryTable}

 0.1    & 14.10.2004 & T. Rantanen  &
 Implementation issues of the cost database were converted from Tommi
 Rantanen's thesis to the design document format.\\

 0.1.1   & 27.04.2006 & A. Cilio    &
 Format revision.\\

\end{HistoryTable}


\tableofcontents



\chapter{INTRODUCTION}

\section{Purpose and Scope}

This document describes the Cost Database of TTA Codesign Environment
(TCE). The document describes how the Cost Database is designed and
the reasons behind the design. The purpose of the Cost Database itself
is explained more detailed in~\cite{EstimatorSpecs}.

The representation of this document is mainly taken
from~\cite{ThesisRantanenTommi} but required differences to the design
are updated.

This document is meant to be a reference to developers and maintainers
who need information on how the Cost Database is designed or how to
use it. In addition, this document is a good source of information for
developers of client applications using the Cost Database.  After
reading the document, the reader should know what are the main
components and their responsibilities as well as what are the main
tasks of the client applications to use the Cost Database properly.

This document does not provide detailed interfaces but descriptions of
the main interfaces of the modules or classes. The interfaces can be
found from the API reference hypertext manual \footnote{
	Currently only from the MOVE API reference.
}.

\section{Definitions}

\begin{description}

\item[Cost Database] %
    Database of cost of the TTA processor building blocks for
    different architectural properties.

\item[Match Type] %
    The condition applied to a field of cost database entries to
    determine whether it matches a given key value.

\item[Search Type] %
    The condition applied to an entry to determine whether it matches
    a given search request.

\end{description}

\section{Acronyms and Abbreviations}

% TODO: update the table

\begin{table}[htb]
\begin{center}
\begin{tabular}{p{0.15\textwidth}p{0.85\textwidth}}
TCE   & TTA Codesign Environment. \\
TTA   & Transport Triggered Architecture. \\
\end{tabular}
\end{center}
\end{table}



\chapter{MODULE OVERVIEW}

\section{``Philosophy of Design''}

The main focus in the design is to implement a flexible and expandable
cost database without ignoring the efficiency. Expandability means
that the following additions are easy and straightforward to implement
to the database:
\begin{itemize}
\item new entry type to the database
\item new field type to a database entry
\item new data type to an entry field
\item new statistic element.
\end{itemize}
In addition, the queries should support different match types, i.e.,
exact match, superset, subset and interpolation, as well as the
following modifications:
\begin{itemize}
\item adding a new match type
\item implementing a completely new search algorithm.
\end{itemize}
The flexibility and expandability requirements mentioned above cannot
conduct the database implementation into an inefficient solution.

\section{High-level design}
\label{sec:database_implem}

\begin{figure}[tb]
\centerline{\psfig{figure=eps/ClassDatabase.eps,width=0.9\textwidth}}
\caption{Class diagram of the cost database.}
\label{fig:class_database}
\end{figure}

The cost database is composed of a few modules and a class as
depicted in Fig.~\ref{fig:class_database}. The \emph{CostDatabase}
class is in a central role in the module. It provides the services for
the clients of the database system. An entry is stored utilizing the
\emph{Entry} module sketched in detail in
Chapter~\ref{cha:data_storage}. However, the most important service of
the database is the \emph{Search()} interface implementing the queries
to the database exploiting the \emph{Query} module.
Chapter~\ref{cha:search_algo} describes the implementation of the
queries, i.e., the class hierarchy to realize a flexible and
expandable search algorithm. Chapter~\ref{cha:input_and_output}
introduces \emph{CostDBReader}, which is a module for implementing the
reader of the \emph{CostDatabase} from a specific file. Furthermore,
Chapter~\ref{cha:client_tasks} represents the actions required from
the clients to make the cost database design to work properly.



\chapter{Data Storage}
\label{cha:data_storage}

The cost database needs to store the entries in a flexible and
expandable way. The class hierarchy used for the data storage of the
database that supports the requirements is depicted in
Fig.~\ref{fig:class_entry}. The \emph{CostDatabase} class contains a
map for storing different entry types. \emph{InsertEntry()} function
can be used to add an entry to the database.

\emph{CostDBEntry} is the main class for representing a cost database
entry. It consists of the search key and the statistics represented by
the \emph{CostDBEntryKey} and \emph{CostDBEntryStats} classes,
respectively. \emph{CostDBEntryStats} class comprises the data for
area, delay, and energy consumption. \emph{CostDBEntryKey}
incorporates the fields of an entry that characterizes it, i.e.,
search key. Nevertheless, the \emph{CostDBEntry} is a
\emph{fa\c{c}ade} for forwarding the interface calls to the complex
subparts, which can be also used independently. This is one of the
purposes of the \textbf{Fa\c{c}ade} design pattern. The advantage is
to group together the statistics and the search key of the database
entry, and to still allow to design them separately. Being the
fa\c{c}ade, the clients of the database entries need only to manage
\emph{CostDBEntry} objects instead of its subparts, hence promoting
weak coupling between an entry and its clients. The clients of the
database entries do not need to be aware of the complex internal
structure if they do not care about it. However, the
\textbf{Fa\c{c}ade} design pattern does not prevent the clients from
using the subpart classes directly if they need to. In fact, this is
required since the use of the search key is needed independently from
the statistics in the client. An alternative for the
\textbf{Fa\c{c}ade} pattern would have been to compose an entry of
each search key field and statistic. Thus, if an entry would have been
used as a search key, the statistics would have always existed as
unwarranted information.~\cite{DesignPatterns}

\begin{figure}[tb]
\centerline{\psfig{figure=eps/ClassEntry.eps,width=1.1\textwidth}}
\caption{Class diagram of the module storing the entries.}
\label{fig:class_entry}
\end{figure}

\emph{CostDBEntryKey}, then, describes the properties of an entry
that are used as a search key. Thus, it is composed of the type of an
entry, and fields, i.e., \emph{EntryKeyField} objects representing a
search key field of an entry. From a \emph{CostDBEntryKey} object,
type and specific fields can be requested.

Each field of an entry key contains some data, which is represented by
the \emph{EntryKeyData} class. \emph{EntryKeyField} class is
composed of the data and the type of the field, which are both
represented by own classes, namely \emph{EntryKeyData} and
\emph{EntryKeyFieldProperty}, respectively. However,
\emph{EntryKeyData} is an abstract base class for different data
types that the field can represent. It defines the interface that the
data types should implement providing the search algorithms a way to
compare different fields. The following data types exist:
\begin{itemize}
\item integer represented by \emph{EntryKeyDataInt} class
\item double represented by \emph{EntryKeyDataDouble} class
\item operation set represented by \emph{EntryKeyDataOperationSet} class.
\end{itemize}
The interface of the \emph{EntryKeyData} provides the clients a
flexible and generic way to handle the data. However, the actual data
value cannot be obtained, which is a drawback. Fortunately, it is not
a huge problem since the clients usually require little information
about the actual data values, if at all. Nevertheless, if the client
needs the data value itself, it can be obtained as a string and
converted to the correct format.

Each database entry, as well as the fields of the entries have a
type. They are represented by the classes \emph{EntryKeyProperty} and
\emph{EntryKeyFieldProperty}, respectively. Class
\emph{EntryKeyProperty} contains static methods for creating and
obtaining certain entry types. The class ensures that only one
instance of a specific entry type exists. Hence, the comparison
between the entry types can be done using pointers, which gives an
important efficiency advantage. Each entry type contains the type
element, i.e., \emph{EntryKeyFieldProperty} instance for each
different field. The policy and the advantage with the field types are
the same as with the entry types, i.e., the \emph{EntryKeyProperty}
class takes care of the fact that each field type has only one
instance.



\chapter{Filtering Search}
\label{cha:search_algo}

Generic search algorithm is one of the main requirements of the cost
database. Since the structure of the database is not stable, the
algorithm cannot depend on a specific field or entry types but it
should be flexible and generic supporting any entry type the client is
able to create. The \emph{Query} module, for which a class diagram is
depicted in Fig.~\ref{fig:class_query}, realizes such a search
algorithm.

The genericness is accommodated already in the highest level where
\textbf{Strategy} design pattern is applied. \emph{CostDatabase}
class contains a reference to the search strategy that is used for
querying the entries from the database according to a specific
search key. The reference points to an instance of the class derived from
\emph{SearchStrategy} which is an abstract base class defining
the interface for all different search algorithms. Using the
\emph{SetSearchStrategy()} function of the \emph{CostDatabase}
object, the client can freely bring desired algorithm on line, i.e.,
dynamic changes of the algorithms are possible, which is one of the
advantages of the \textbf{Strategy} pattern. Due to the
\textbf{Strategy} pattern, unnecessary conditional statements are
completely avoided. In addition, the pattern completely hides
complex, algorithm-specific data structures from its
clients. \cite{DesignPatterns}

The responsibility of the \emph{CostDatabase} in the queries is only
to forward the search request, i.e., the search key and search type,
to the active strategy. Moreover, the \emph{CostDatabase} object adds
the database entries for the \emph{SearchStrategy} as the data from
which to search certain entries. The type of the match applied to a
specific entry field is denoted as \emph{match type}. It is
represented by class \emph{MatchType}. The type of the whole search
of an entry is called \emph{search type}. It is composed of the list
of match types, which, when correctly made, contains a match type for
each field of an entry.

\begin{figure}[tb]
\centerline{\psfig{figure=eps/ClassQuery.eps,width=1.0\textwidth}}
\caption{Class diagram of the module implementing the queries to the database.}
\label{fig:class_query}
\end{figure}

\section{Principles}

One search strategy, class \emph{FilterSearch} in
Fig.~\ref{fig:class_query}, is implemented based on the filtering of
unwanted entries out from the results. Entries that do not match with
the search key in an entry field according to the used match type
cannot be a match for the query, and hence can be removed from the
results. Continuing this procedure for all fields results into an
entry collection which matches the request. If more than one entry
exist in the results, the client is responsible for enlarging on the
collection of the appropriate entries further.

The division of the search to the smaller subparts is based on the
\textbf{Strategy} pattern. \emph{Matcher} is the abstract base
class, i.e., the \emph{strategy} defining the interface for the
filters of single entry fields. Since the type of the filter must be
flexibly changed, the \textbf{Strategy} design pattern is extremely
appropriate for this context offering the possibility to change the
concrete strategies dynamically~\cite{DesignPatterns}. For each
\emph{MatchType}, an own \emph{Matcher} will be derived to implement
specific type of filtering.

\begin{algorithm}[b]
\caption{FilterSearch::Search(\param{search\_key}, \param{entries},
                              \param{search\_type})}
\label{alg:filter_search}
\begin{algorithmic}[\algolinenumbering]
\STATE create \var{ML} (list of \emph{Matcher}s) from the \param{search\_type}

\FORALL{matchers \var{M} in the \var{ML}}
  \STATE \var{M}.Filter(\param{search\_key}, \param{entries})
\ENDFOR
\STATE \algoreturn entries
\end{algorithmic}
\end{algorithm}

The high-level algorithm of the filtering search is represented in
Alg.~\ref{alg:filter_search}. The search algorithm takes three
parameters as input: the search key, i.e., the characteristics of an
entry to be searched for, the list of entries from which to find the
matches, and the search type. In the beginning of the algorithm,
instances of the subclasses of \emph{Matcher} are created according
to the requested search type, which is indicated by the list of
\emph{MatchType} instances.  Thereafter, the filtering of each
\emph{Matcher} object is applied to the list of database entries
using the search key. The \emph{Matcher} object itself knows the
field into which apply the filtering. In the end of the represented
algorithm, the entries contain only appropriate database items
accepting the search criteria, i.e., search key with certain search
type.

\section{Subalgorithms}

The cost database supports four different match types for a field:
exact match, superset, subset and interpolation. Thus, own class
derived from the base class \emph{Matcher} exists for each of them to
implement subparts of the filtering search. They implement a specific
behaviour to filter out entries being inappropriate in a specific
field according to the requested match type.

Queries requiring equal results for a specific field are using a
match type called exact match. The algorithm for filtering according
to exact match is represented in Alg.~\ref{alg:exactmatch_filter}. It
checks whether an entry has an equal value with the search key in a
specific field or not.

Superset is a match type for querying greater field value or a
superset of the search key for a specific field. The filtering
algorithm is illustrated in Alg.~\ref{alg:superset_filter}. In
the following, the algorithm is explained in detail:
\begin{itemize}
\item 1: Scroll through the whole input entry list.
\item 4-6: If the entry is not equal or greater than the search key
      further processing is not required and the entry can be ignored.
\item 7: The results already found by the algorithm are browsed through.
\item 8-10: The result entry is not handled furthermore if it does not
      belong to the \emph{same group of entries}, i.e., has equal field
      values in each field except in the key field for which
      the superset algorithm is applied to.
\item 12-13: If the current entry is smaller than the current result
      entry, the resulting entry can be deleted from the results list.
\item 14-15: If the current entry is greater than the current result
      entry, the result collection already contains a better
      alternative than the current entry which will not be added to
      the results later on.
\item 18-20: If the results do not contain an entry of this group,
      it will be added. The entry will be added also if it is the best
      entry in this group that is found so far.
\item 22: Finally, the resulting entries are assigned into the output.
\end{itemize}

\begin{algorithm}[b]
\caption{ExactMatch::Filter(\param{search\_key}, \param{entries})}
\label{alg:exactmatch_filter}
\begin{algorithmic}[\algolinenumbering]

\FORALL{entries \var{E} in \param{entries}}

  \STATE \var{E\_field} = \var{E}.KeyFieldOfType(\param{Matcher::field\_type})

  \IF{\var{E\_field}.IsEqual(\param{search\_key})}
    \STATE \var{results}.Add(E)
  \ENDIF

\ENDFOR

\STATE \param{entries} = \var{results}

\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tb]
\caption{SuperSet::Filter(\param{search\_key}, \param{entries})}
\label{alg:superset_filter}
\begin{algorithmic}[\algolinenumbering]

\FORALL{entries \var{E1} in \param{entries}}

  \STATE \var{add\_entry} = true
  \STATE \var{E1\_field} = \var{E1}.KeyFieldOfType(\param{Matcher::field\_type})

  \IF{!(\var{E1\_field}.IsEqual(\param{search\_key}) or
        \var{E1\_field}.IsGreater(\param{search\_key}))}
    \STATE continue
  \ENDIF

  \FORALL{entries \var{E2} in \var{results}}

    \IF{!OnlyThisFieldDiffers(\param{Matcher::field\_type}, \var{E2}, \var{E1})}
      \STATE continue
    \ENDIF

    \STATE \var{E2\_field} = \var{E2}.KeyFieldOfType(
                                 \param{Matcher::field\_type})

    \IF{\var{E1\_field}.IsSmaller(\var{E2\_field})}
      \STATE \var{results}.Delete(E2)
    \ELSIF{\var{E1\_field}.IsGreater(\var{E2\_field})}
      \STATE \var{add\_entry} = false
    \ENDIF

  \ENDFOR

  \IF{\var{add\_entry}}
    \STATE \var{results}.Add(\var{E1})
  \ENDIF

\ENDFOR

\STATE \param{entries} = \var{results}

\end{algorithmic}
\end{algorithm}

Subset search type is used for finding smaller field value or a subset
of the search key for a specific entry field. The principles and the
algorithm for subset filtering are the same as for the
superset. However, \emph{IsSmaller()} function calls are replaced with
\emph{IsGreater()} calls and vice versa.

The flexibility requirements for the database queries demand that
linear approximation of the statistics should be possible to use if
the behavior of a hardware resource for a certain characteristic is
accordant with that. Therefore, interpolation match type, for which
the filtering algorithm is depicted in
Alg.~\ref{alg:interpolation_filter}, exists. The algorithm is much
more complicated than for other search types due to the genericness
requirements of the search algorithm and complexity of the
interpolation.

\begin{algorithm}[tb]
\caption{Interpolation::Filter(\param{search\_key}, \param{entries})}
\label{alg:interpolation_filter}
\begin{algorithmic}[\algolinenumbering]

\FORALL{entries \var{E} in \param{entries}}

  \STATE \var{new\_pair} = true
  \STATE \var{E\_field} = \var{E}.KeyFieldOfType(\param{Matcher::field\_type})

  \FORALL{pairs \var{P} in \var{pairs}}
    \IF{!OnlyThisFieldDiffers(\param{Matcher::field\_type},
                              \var{P}.smaller, \var{E})}
      \STATE continue
    \ENDIF

    \IF{\var{E\_field}.IsEqual(\param{search\_key})}
      \STATE \var{P}.smaller = E
    \ELSIF{\var{E\_field}.IsSmaller(\param{search\_key}) and
           \var{E\_field}.IsGreater(\var{P}.smaller)}
        \STATE \var{P}.smaller = E
    \ELSIF{\var{E\_field}.IsGreater(\param{search\_key}) and
           \var{E\_field}.IsSmaller(\var{P}.greater)}
        \STATE \var{P}.greater = E
    \ENDIF

    \STATE \var{new\_pair} = false
    \STATE break
  \ENDFOR

  \IF{\var{new\_pair}}
    \IF{\var{E\_field}.IsEqual(\param{search\_key}) or
        \var{E\_field}.IsSmaller(\param{search\_key})}
      \STATE \var{pair}.smaller = E
    \ELSE
      \STATE \var{pair}.greater = E
    \ENDIF

    \STATE \var{pairs}.Add(pair)    
  \ENDIF

\ENDFOR

\FORALL{pairs \var{P} in \var{pairs}}
  \IF{\var{P}.smaller.KeyFieldOfType(\param{Matcher::field\_type}).IsEqual(
                                                          \param{search\_key})}
    \STATE \var{results}.Add(\var{P}.smaller)
  \ELSIF{\var{P}.HasBothElements()}
    \STATE \var{results}.Add(Combine(\var{P}.smaller, \var{P}.greater,
                                     \param{search\_key})
  \ENDIF
\ENDFOR

\STATE \param{entries} = \var{results}

\end{algorithmic}
\end{algorithm}

The filtering algorithm in the interpolation is composed of two
phases. In the first phase (lines 1-26), \emph{entry pairs} composed
of a smaller and a greater entry are constructed from the entry list
given as an input for the algorithm. Both the smaller and the greater
belong to the same group of entries. The smaller entry in the pair
embodies an entry which has a smaller value than the search key in the
field for which the search is applied to. The greater entry has
similar meaning, i.e., representing greater value. The smaller entry
of a pair \emph{P} is represented as \emph{P}.smaller and the greater
entry as \emph{P}.greater in Alg.~\ref{alg:interpolation_filter}. In
the second phase (lines 27-34), each pair is combined together to form
one entry having linearly approximated area, energy and timing
statistics.

In the following, the whole interpolation algorithm represented in
Alg.~\ref{alg:interpolation_filter} is explained in detail line
by line:
\begin{itemize}
\item 1-4: The creation of the entry pairs starts by scrolling through
      the whole input entry list. Inside that, the pairs already found
      are browsed through.
\item 5-7: The pair is not handled furthermore if it does not belong
      to the same group of entries.
\item 8-14: The current entry pair is updated if the current entry is
      better than the smaller or the greater entry in the pair.
\item 15: If the algorithm got through the test on line 5, new
      entry pair is not required after the loop since a pair exists
      already for this group.
\item 16: The loop can be finished, since only one pair exists for a
      certain group.
\item 18-26: If an entry pair does not exist for the current entry, it
      has to be created. Current entry is assigned for the smaller or
      greater entry of the new pair, depending on whether it is
      smaller or greater than the search key.
\item 27: Each pair that were found are scrolled through.
\item 28-29: If the pair is composed of an entry that is equal to the
      search key, it is added to the results without any
      treatment.
\item 30-31: If both smaller and greater entry exist in the pair, they
      will be combined into one entry. Otherwise, the pair is
      inappropriate and it will be ignored.
\item 34: Finally, the resulting entries are assigned into the output.
\end{itemize}

The combination of the smaller and greater entry is made by linearly
approximating area, energy, and timing according to the difference of
the entry fields from the search key. For example, if the bit width
requested is 20 and the smaller entry has 16 as the bit width and the
greater 32, the requested bit width is 25\% of the total gap between
the bit widths. If the area of the smaller entry is 200 gates and 350
for the greater, the requested are will be 237,5 gates. Going into the
details, the function \emph{EntryKeyData}::\emph{Coefficient()} in
the \emph{Entry} module returns the coefficient required by the
linear approximation of the area, energy, and timing. In the example
above, it would have returned 0.25.

Since the energy values exist for different utilizations of a hardware
component, the combination of the energy is slightly more
complicated. The requested entry contains energy value for each
utilization in the both smaller and the greater entry. For example, if
one entry contains energy values for utilizations 0,1; 0,6 and 0,9, and
another one for 0,2 and 1,0, the entry obtained by combining these two
entries would contain energy data for utilizations 0,1; 0,2; 0,6; 0,9
and 1,0. Energy for each utilization is obtained similarly to the area
and timing.

\section{Optimizations}

The algorithm represented in Alg.~\ref{alg:filter_search} can be
optimized to make the queries much more efficient. Two optimization
methods have been used in the optimized version represented in
Alg.~\ref{alg:filter_search_optimized}: \emph{cache} and
\emph{quick filtering}.

The TTA processors have several similarities. All the buses of the
processor configuration are usually identical as well as RFs, and
input and output sockets. FUs have more differences than the other
resources since they support different operation sets. The purpose of
the database is to provide statistics for the estimator, which
evaluates the costs of a processor configuration. Since the processor
is composed of several similar resources, the database encounters
identical query requests. Thus, filtering search supports
\emph{cache}, i.e., results of the previous queries can be quickly
used if identical search is requested. In the algorithm depicted in
Alg.~\ref{alg:filter_search_optimized}, the cache appears in the
beginning (lines 1-3), where the cache is checked whether it already
contains the results for this query. In the end of the algorithm,
i.e., on line 11, the results of the new query are added to the cache.

However, the size of the RF as well as the number of read and write
ports of the RF may vary especially in the design space explorer. In
addition, the fanin of the input sockets and fanout of the output
sockets contains also some variations in the connectivity optimization
of the explorer. Thus, the advantage of the cache cannot always be
fully utilized. Nevertheless, it has a significant decreasing impact
on the query times of the application.

The cost database usually consists of several hundreds, or even
thousands of entries. Only a few of them satisfies the requirements of
the search request, and most of them are completely inappropriate when
quickly checking their properties. In addition, performing filtering
of the most complex match types for the huge entry collections takes a
quite long time. Due to these facts, an optimization called
\emph{quick filtering} is used for the database queries. It includes a
filtering of unwanted entries out of the resulting entry collection
before calling the actual filtering. In the
Alg.~\ref{alg:filter_search_optimized}, lines 5-7 depicts the usage of
the quick filtering, which is performed for each \emph{Matcher}
instance before any of the actual filterings take place in line 9. The
speed of a quick filter call must be in order of growth $O$(\var{n}),
where \var{n} is the number of entries passed to the quick filter
algorithm. The filtering itself can be of any order of growth, since
it must result in an entry collection satisfying the requirements of
the search request.

\begin{algorithm}[b]
\caption{FilterSearch::Search(\param{search\_key}, \param{entries},
                              \param{search\_type})}
\label{alg:filter_search_optimized}
\begin{algorithmic}[\algolinenumbering]

\IF{\var{cache}.Check(\param{search\_key}, \param{search\_type})}
  \STATE \algoreturn \var{cache}.Entries(\param{search\_key},
                                         \param{search\_type})
\ENDIF

\STATE create \var{ML} (list of \emph{Matcher}s) from the \param{search\_type}

\FORALL{matchers \var{M} in the \var{ML}}
  \STATE \var{M}.QuickFilter(\param{search\_key}, \param{entries})
\ENDFOR

\FORALL{matchers \var{M} in the \var{ML}}
  \STATE \var{M}.Filter(\param{search\_key}, \param{entries})
\ENDFOR

\STATE \var{cache}.Add(\param{search\_key}, \param{search\_type},
                       \param{entries})

\STATE \algoreturn entries
\end{algorithmic}
\end{algorithm}

The filtering algorithms represented earlier are divided into quick
filtering and actual filtering algorithms for the optimized version of
the filtering search. The filtering of exact match is $O$(\var{n}) in
the order of growth. Thus, it can be moved to the quick filtering
function as it is, and the actual filtering remains empty. In the
interpolation, an entry cannot be removed from the results without
having information about other entries. Hence, nothing can be done in
the quick filtering phase and the filtering algorithm is the same as
it is without quick filtering. Some filtering can be done for superset
search in the quick filtering phase. Each entry containing equal or
greater value in the field under interest must be left in the results
and other entries can be removed as illustrated in
Alg.~\ref{alg:superset_quick_filter}. The algorithm does not work if
it removes entries with smaller value from the results, since for some
data types, such as sets, a value may be neither smaller nor greater
than another value.

The filtering algorithm of the superset search can be slightly
modified since the algorithm can trust that the quick filtering
represented in the Alg.~\ref{alg:superset_quick_filter} is already run
before the filtering itself is executed. Thus, each entry in the input
is equal or greater than the search key and lines 4-6 of the superset
algorithm represented in Alg.~\ref{alg:superset_filter} can be
removed. Furthermore, it can be noticed that comparison with the
search key is not needed anymore, since each entry is equal or greater
than the search key after the quick filter call. Similar changes can
be done for subset search when quick filtering is used.

In general, the quick filtering speeds up the queries a lot, since the
size of the entry collection passed to the filtering itself reduces
significantly. Of course, in a very specific case the query time may
increase because of quick filtering. For example, if an entry having
the greatest value of the whole database on a field for which subset
match type is applied, is searched, the quick filtering algorithm of
the subset is run and no entry is removed from the results.

\begin{algorithm}[tb]
\caption{SuperSet::QuickFilter(\param{search\_key}, \param{entries})}
\label{alg:superset_quick_filter}
\begin{algorithmic}[\algolinenumbering]

\FORALL{entries \var{E} in \param{entries}}

  \STATE \var{E\_field} = \var{E}.KeyFieldOfType(\param{Matcher::field\_type})

  \IF{\var{E\_field}.IsEqual(\param{search\_key}) or 
      \var{E\_field}.IsGreater(\param{search\_key})}
    \STATE \var{results}.Add(E)
  \ENDIF

\ENDFOR

\STATE \param{entries} = \var{results}

\end{algorithmic}
\end{algorithm}

\section{Analysis}

The filtering algorithm and its subalgorithms are extremely generic
and flexible. They are completely independent of the entry types,
field types of the entries and data types of the entry fields. Thus,
these properties of the database can be varied by the client as much
as is required and the search algorithm still works.

Due to the general interface of the data, the search algorithms can
handle any kind of data. Thus, the database entries can contain any
kind of data if it implements \emph{EntryKeyData} interface. However,
a filtering algorithm may require some new, search-specific
interfaces. For example, interpolation requires
\emph{EntryKeyData}::\emph{Coefficient()} function, which is not
required by other filtering algorithms. Nevertheless, operation set
type cannot implement this function, which implies that interpolation
cannot be applied to the fields containing operation set type of
data. If the client tries to use interpolation match type for
operation set type of data, a run-time error will occur preventing the
search algorithm from giving incorrect and meaningless results. On the
other hand, the whole comparison interface of the \emph{EntryKeyData}
class is specific for the queries. \emph{IsEqual()} function is
required by the exact match filtering as well as the other match
types. In addition, \emph{IsSmaller()} and \emph{IsGreater()}
functions are needed to make superset and subset filterings
possible. However, these three functions can be considered as the
basic set for making any kind of requests to the database.

The search type is specified by a list of \emph{MatchType}
objects. The supported match types are exact match, superset, subset
and interpolation. Any combination of them can be used to form the
search type. Particularly, multi-dimensional interpolation is flexibly
possible. However, some client may be interested in any field values,
i.e, to not filter any entries out of the results according to a
specific field. This functionality can be obtained by using a special
match type called \emph{match all}.

The genericness and flexibility of the queries gives a huge advantage
since the format of the database is not static. Nevertheless, it has
always the drawback of being a bit inefficient. If the efficiency of
the database queries becomes an issue, faster algorithms need to be
considered. More speed can be achieved, for example, by making
assumptions of the entry types or field types of the entries. The
filtering search algorithm does not perform any checks for the values of
other fields but the field under filtering. Adding more dependencies
between the fields can give some efficiency advantage to the exclusion
of genericness.



\chapter{Input and Output}
\label{cha:input_and_output}

This Chapter contains the ideas for the reader/writer of the cost
database. \emph{The design and implementation is to be done.}

I/O functionality of the cost database can be made totally independently
from the cost database implementation, that is, data structures and
search algorithms do not depend on the way the cost database is stored
permanently.

The database is stored in the SQL database. To utilize the search
algorithms represented in Chapter~\ref{cha:search_algo} the database
has to be converted into the data structures represented in
Chapter~\ref{cha:data_storage}.

An alternative for this is to apply Serializable system already
existing in TCE. A new serializer must be implemented for SQL and
\emph{CostDBSearializer} should be derived from it.

Do we need the writer part at all, that is, it might be that we never
need to convert the data structures represented in
Chapter~\ref{cha:data_storage} to permanent storage utilizing SQL
storage format.



\chapter{Client's Tasks}
\label{cha:client_tasks}

\begin{figure}[tb]
\centerline{\psfig{figure=eps/ClassClient.eps,width=0.8\textwidth}}
\caption{Class diagram of the hardware cost estimator.}
\label{fig:class_client}
\end{figure}

This Chapter describes the actions required to be made by the
\emph{Client} when using the cost database properly.
Figure~\ref{fig:class_client} depicts the relations that the
\emph{Client} has to the cost database classes and modules. The
functionality implemented in the \emph{Client} consists the following
steps:
\begin{enumerate}
\item Create entry types, i.e., \emph{EntryKeyProperty} instances,
      and entry field types, i.e., \emph{EntryKeyFieldProperty}
      instances, for the cost database.
\item Read the cost database.
\item Create and assign a search strategy, i.e., \emph{FilterSearch}
      instance, to the cost database.
\item Create the search type of the queries, i.e., \emph{MatchType}
      list, for each entry type.

\item Obtain the type of an entry, i.e., \emph{EntryKeyProperty}
      instance, to be evaluated.
\item Create the search key, i.e., \emph{CostDBEntryKey} instance
      containing \emph{EntryKeyField} objects constructed as follows:
      \begin{enumerate}
      \item Obtain the type of an entry field, i.e., 
            \emph{EntryKeyFieldProperty} instance.
      \item Create an instance of the appropriate class implementing
            the interface \emph{EntryKeyData}.
      \item Create \emph{EntryKeyField} object.
      \end{enumerate}
\item Perform a database query.
\item Process the resulting entry collection furthermore, i.e., select
      the correct entry from the results if more than one exist as
      well as the correct statistics from the selected entry if more
      than one statistics exist.

\end{enumerate}
Steps 1-4 are actions required by the cost database from its clients,
before the database queries can properly take place. In steps 5-8,
the actual query is performed. Steps 5 and 6 prepares for the database
query performed in step 7. Steps 8 includes client-specific further
processing of the results obtained from the database. In step 8, the
client can freely choose an entry from the results if there are more
than one entry. In the search algorithm, the entries are chosen based
on the characteristics. However, the client can choose an entry from
the results using any decent principle based on the characteristics or
statistics. For example, an FU query might give as a result multiple
entries which support different operation sets. For example, the
smallest area or the smallest energy consumption can be chosen
depending on which properties are preferred.



\chapter{REJECTED ALTERNATIVES}

-



\chapter{IDEAS FOR FURTHER DEVELOPMENT}

-



\chapter{PENDING ISSUES}

\begin{description}

\item[14.10.2004] The set of required data types, that is, classes derived
  from \emph{EntryKeyData}, is unknown until the final contents of the cost
  database will be published. Integer and double types can be obtained from
  MOVE but at least operation set has to be implemented. In addition, if
  some kind of optional implementation string appears to the database, a
  data type for it has to be implemented. That type would be, for example,
  \emph{EntryKeyDataImplementation}, and it would have an ``empty'' value
  which is equal to any other value. This will model optional field.
  ---~T.~Rantanen

\end{description}



\chapter{MAINTENANCE}

\section{Data Storage}

The design of the data storage part of the cost database represented
in Chapter~\ref{cha:data_storage} is extremely flexibile and
expandable. Adding a new type of entry or entry field does not require
any changes to the database implementation, i.e., the client only has
to create and use the new types. Adding a new type of data included in
the entry field is extremely easy, i.e., the new class should be
derived from the base class \emph{EntryKeyData} and the whole
interface of it should be overloaded.

\section{Search Algorithm}

Basically, the existing search algorithm, that is, filtering search,
do not need to be changed at all. If the algorithms implemented do not
satisfy some requirements, they can be easily changed. The whole
filtering algorithm can be changed by deriving a new algorithm from
the interface class \emph{SearchStrategy} and by changing the client
to use it. Moreover, adding a new match type is trivial. Filtering
search can support the new match type by deriving a new class from the
\emph{Matcher} base class.


% ------------------------------------------------------------------------
%  References are generated with BibTeX from a bibtex file.
\bibliographystyle{alpha}
\cleardoublepage

%% Equivalent to a chapter in the table of contents
\addcontentsline{toc}{chapter}{BIBLIOGRAPHY}
\bibliography{Bibliography}


\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
