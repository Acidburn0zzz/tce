\documentclass[a4paper,twoside]{tce}
\usepackage{pslatex}



\begin{document}
\author{Andrea Cilio, Pekka J‰‰skel‰inen}
\title{Memory Model Module}
\ver{0.8.3}
\firstday{10.03.2004}
\lastday{28.04.2006}
% id number in S- sequence
\docnum{014}
% draft/complete/committed
\state{draft}

\maketitle



\chapter*{Document History}

\begin{HistoryTable}

 0.1    & 10.03.2004 & A. Cilio   &
 First fragments.  Migrated API definition from Simulator design notes. \\

 0.2    & 30.03.2004 & A. Cilio   &
 Completely revised.  Added many pending issues.\\

 0.2.1  & 19.05.2004 & P. J‰‰skel‰inen &
 Added the \emph{MemoryArea} helper class and \emph{area} method.\\

 0.2.2  & 19.05.2004 & A. Cilio &
 Review of \emph{MemoryArea} description and API.\\

 0.3.0  & 13.08.2004 & P. J‰‰skel‰inen &
 Updates according to email discussions and review. \\

 0.4    & 18.08.2004 & A. Cilio & 
 Review of last changes.  Rewritten alignment, introduced word order
 parameter. \\

 0.4.1  & 19.08.2004 & P. J‰‰skel‰inen &
 Moved ``using \emph{MemoryArea} to implement MEM macro of OSAL'' to OSAL
 design document.\\

 0.5    & 31.08.2004 & A. Cilio &
 Major update after review with Pekka and Jarmo. Resolved pending issues.
 Added unique request code, removed locking API and methods to check for
 locks-on-trigger.  Complete revision of \emph{MemoryArea}. \\

 0.6    & 14.09.2004 & A. Cilio &
 Renamed interfaces of memory-related classes. Corrected \emph{MemoryArea}.
 Changed document structure.\\

 0.7    & 07.10.2004 & A. Cilio &
 Revised Module Overview. Minor changes.\\

 0.8    & 15.10.2004 & A. Cilio &
 Replaced \emph{MemoryArea} with \emph{TargetMemory}. Modified the interface
 of \emph{Memory}.\\

 0.8.1  & 28.10.2004 & J. Nyk‰nen &
 Added \emph{advanceClock} method to \emph{TargetMemory}.\\

 0.8.2  & 21.04.2006 & A. Cilio &
 Minor cleanup. \\

 0.8.3  & 28.04.2006 & A. Cilio &
 More cleanup. \\
\end{HistoryTable}


\tableofcontents



\chapter{INTRODUCTION}

\section{Purpose and Scope}

This document specifies the Application Programming Interface of the model
used to represent the memory system accessed by TTA processors.  The TCE
toolset distribution includes an implementation of such memory model, in
this document simply referred to as Memory Module.  This document also
describes the design of the Memory Module at a level of detail such that
implementation of the Memory Module based on it will be straightforward.

Other implementations of the Memory Model (External Memory Modules) are
possible.  These implementations are out of the scope of this document.

This document is intended for implementers of the Memory Module (or an
External Memory Module) and those who must develop clients of the Memory
Model.  In addition to this document, the reader is referred to the
automatically-generated API reference document [[refs]] for a description of
the Memory Module API.  This document does not discuss purpose or reasons
behind the Memory Model on which the design is based; this information is
found in the Simulator Functional Specifications~\cite{SimulatorSpecs}.

\section{Definitions}

\begin{description}
\item[External Memory Module]%
  A module that implements the Memory Model API and is not built on top of
  the TCE code base.
\item[Memory Module]%
  Implementation of the Memory Model API provided together with the TCE
  toolset.
\end{description}

\section{Acronyms and Abbreviations}

\begin{table}[htb]
\begin{center}
\begin{tabular}{p{0.15\textwidth}p{0.85\textwidth}}
API   & Application Programming Interface. \\
FIFO  & First-In, First-Out. \\
MAU   & Minimum Addressable Unit (of a memory address space). \\
MSM   & Machine State Model.\\
OSAL  & Operation Set Abstraction Layer. \\
TCE   & TTA Codesign Environment. \\
TTA   & Transport Triggered Architectures. \\
URC   & Unique Resource Code.\\
\end{tabular}
\end{center}
\end{table}



\chapter{MODULE OVERVIEW}

\section{``Philosophy of Design''}

The purpose of the memory model module is to define an Application
Programming Interface (API) that can be used by all clients of TCE toolset
that need to emulate the access to the memory system of a target TTA
processor.

The main principles behind the architecture of the memory module are:
\begin{enumerate}
\item %
  Support for cycle-accurate emulation of memory operations.  This
  requirement implies means to communicate when a data word expected from
  memory is not ready or when an access request cannot be accepted (the
  memory is ``busy'').  Also, the memory model must provide an interface to
  receive notifications of clock cycle strokes.
\item %
  Support for lower-level, transparent access to memory contents.  This
  requirement is critical to allow flexible use of the memory module by
  clients such as, for example, a simulator/debugger.
\item %
  The interface must be suitable for future extensions to the TCE toolset
  that affect the maximum supported size of the MAU or the alignment.
\end{enumerate}

For detailed motivations for above outlined requirements the reader is
referred to Chapter 4 of~\cite{SimulatorSpecs}.

\subsection{Fundamental Restrictions and Guidelines}
\label{ssec:restrictions}

The design of the memory model must adhere to a fundamental restriction:
\begin{quote}
  The Memory Model treats all requests coming in the same clock cycle
  independently of the order in which are made.
\end{quote}


\section{External Modules}

% Overview of module from an external perspective:
% - dependencies with external modules-subsystems
% - IFs of external modules that are used
% - external (file) data formats used

% Do not put here the interfaces provided to other modules.

The Memory Model is one of the root classes of the TCE domain.  It does not
depend on any other class.

The Memory Module, however, depends on helper class \emph{MemoryContents}
(Section~\ref{sec:memcontents}) to implement the actual contents of the
memory.

The Memory Model itself does not depend on target-oriented base data types,
%
\note{currently, target-oriented types are in toolkit, could be moved to
  domain module in the future}
%
that is, integer and floating-point data types used by the modules
and applications that model the target processor.

The adaptation from the low-level Memory Model interface to the
domain-friendly interface based on target-oriented built-in data types is
implemented in class \emph{TargetMemory} (see Section~\ref{sec:targetmem}),
which is an instance of the \emph{Adaptor} design
pattern~\cite{DesignPatterns}.

Class \emph{TargetMemory} is part of the package that contains the Memory
Module and is built on top of the Memory Model.  Unlike the other classes of
the package, \emph{TargetMemory} depends also on the TCE toolkit library,
which provides byte order information on the host computing system.

\section{Architecture of Databases}

The only base of data of the Memory Module is the data stored in the memory
it models.  The memory contents is implemented in a separate class,
\emph{MemoryContents} (see Section~\ref{sec:memcontents}).


\section{Main Restrictions of Implementation}

Even if the Memory Module is a piece of the TCE simulation subsystem, it
must not contain any dependency to the Machine State Model or domain
objects.

\section{Module Communication}

Since the Memory Model does not depend from any other domain object, there
is no communication from Memory Model to other domain object.



\chapter{Module Design}

\section{Overview}

\begin{figure}[tb]
\centerline{\psfig{figure=eps/MemModule.eps,width=0.75\textwidth}}
\caption{Main classes of the memory module and their main clients.}
\label{fig:mem-modules}
\end{figure}

The Memory Module consists of several classes. The main class,
\emph{Memory}, defines the Memory Model interface and provides a very
low-level view of a target memory system. Class \emph{TargetMemory}
implements a more ``client-friendly'' interface of the memory system. This
interface is based on target-oriented built-in data types. Class
\emph{MemoryContents} is a convenient building block for representing the
actual data stored in a Memory Model. Figure~\ref{fig:mem-modules} offers a
view of the main classes of the Memory Module and their dependencies with
client modules.

\section{Interface}

% High level description of the interface of this module.  Low level details
% are possible, but should be restricted to simple classes (like, for
% example, NodeDescriptor in mdf module).  Low level details of large
% interfaces, if present at all, should be stored in a separate UML
% specification file (for example, Rational Rose MDL).

\subsection{Architectural Interface}
\label{ssec:mem-apiarch}

The architectural interface provides methods for emulating the
cycle-accurate behaviour of accesses to data memory due to operations of the
target architecture such as load and store.

\paragraph{Identification of sources of requests.}
Requests to memory coming from the same source can be identified by means of
a unique request code. This numeric code, called Unique Resource Code (URC),
identifies the source of the request (a client of memory). When a method
that expects a URC is called without, a reserved request code is used in
stead.

\begin{description}
\item[Memory(start : Word, end : Word, MAUSize : int, wordSize : int,
  align : int)]%
  Create the model of a memory mapped in an address space with the given
  parameters.  Parameters \emph{start} and \emph{end} define the bounds of
  the address space, \emph{MAUSize} defines the bit width of the minimum
  addressable unit, \emph{wordSize} gives the size of the natural word as
  number of MAUs. Finally, \emph{align} gives the alignment constraint for
  natural words.
  
  The alignment setting is the number with which the addresses of natural
  words must be dividable.  For example, if alignment is 2, natural words at
  addresses 0 and 2 can be accessed directly, but words at address 1 or 3
  cannot.  Assuming that the natural word is 2 MAUs, to access a word at
  address 1, the memory client must read two natural words, one at address 0
  and one at address 2, and then combine, respectively, their lower and
  upper halves.

\item[initiateRead(address : Word, size : int, id : URC)]%
  Initiates a load request for \emph{size} minimum addressable units (MAUs)
  starting from address \emph{address}.  Returns nothing.  If the request
  cannot be initiated, the memory becomes unavailable after the clock cycle
  ends. Requests to memory coming from the same source are identified by
  means of the unique request code \emph{id}.

\item[loadData(data : MAUVector\&, id : URC)]%
  Obtains the number of MAUs specified by a pending load access request
  from the request source identified by \emph{id}.  The vector value is
  undefined if the result of the requests is not ready.  Calling this method
  repeatedly does not change the result nor affects the Memory Model state
  until another clock cycle is elapsed.

  If
  %
  \note{DISCUSS}
  %
  the given vector has a size that does not correspond to the number of
  MAUs of the read request, the vector is resized.

\item[isResultReady(id : URC) : bool]%
  Returns `true' if the result of the oldest pending request from the
  request source identified by \emph{id} is ready and can be read by one of
  the \verb#readData()# methods.  Calling this method repeatedly does not
  change the result nor affects the Memory Model state until another clock
  cycle is elapsed.

\item[initiateWrite(address : Word, data : MAUVector, size : int, id : URC)]%
  Initiates a store request for \emph{size} minimum addressable units
  (MAUs) in vector \emph{data},
  %
  \note{DISCUSS: what if vector size $<$ size argument?}
  %
  starting from memory address \emph{address}.  Returns nothing.  If the
  request cannot be initiated, the memory becomes unavailable after the
  clock cycle ends. Requests to memory coming from the same source are
  identified by means of the unique request code \emph{id}.

\item[advanceClock()] %
  Inform the Memory Model that a clock cycle has passed.  The Memory Model
  may thus compute the next late-coming results of pending load access
  requests and update its available/unavailable state to the requests
  received in the last clock cycle.
\end{description}

A load operation is performed in three phases:
\begin{enumerate}
\item %
  Request load access: \verb#initateRead()# is called.
\item %
  Check whether the result of the load access request is ready by calling
  \verb#isResultReady()# until it returns `true'. This method must be called
  in every cycle in which a new result has arrived, otherwise the result
  value could be lost, overwritten by results coming in later cycles.
\item %
  Read loaded data by calling \verb#loadData()#.  This methods should be
  called only when \verb#isResultReady()# returns `true', otherwise the
  value of the data read is undefined.
\end{enumerate}

\paragraph{Ordering constraints.}
For correct working of the Memory Model, calls to \verb#loadData()# for
different pending requests coming from the same source and with the same
address must take place in the same order in which the corresponding
requests were initiated. This restriction comes from an equivalent
restriction in the Memory Model: no two pending requests from it can be
satisfied out of order.

\subsection{Locking}

The memory provides a method to signal whether all access requests started
in previous cycles could be accepted or not. In the latter case, the memory
is unavailable and the client should pause normal operations and enter a
``wait state'' until the memory becomes available again.

\begin{description}
\item[isAvailable() : bool]%
  Returns `true' if all access requests received in previous cycle have been
  accepted and thus clients can proceed with normal operation.  When the
  memory is not available, clients should enter a wait state where not only
  new access requests cannot be sent to memory, but even the normal
  operations should be halted.
\end{description}

Tests for memory availability are needed by clients of Memory model that
simulate (part of) a processor core.  For example, a processor controller
that receives `false' from \verb#isAvailable()# should globally lock
execution, and freeze the fetch-decode-execute chain.

\subsection{Alignment}

The alignment restrictions (if any) are handled by the Memory Model 
implementation. Clients (OSAL operations) do not have to check for 
alignments.

\subsection{Service Interface for Data Transfers}
\label{ssec:mem-apiservice}

The Memory Model provides an interface to transfer data to and from the
memory immediately and ignoring lock conditions and resource constraints.
\begin{description}
\item[readBlock(address : Word, data : MAUVector\&)]%
  Copy a memory area starting at address \emph{address}, as many minimum
  addressable units (MAUs) long as the size of vector \emph{data}.
\item[writeBlock(address : Word, data : MAUVector)]%
  Copy a vector of MAUs \emph{data} into the memory area that starts at
  address \emph{address}. The number of MAUs is given by the size of the
  vector.
\end{description}


\section{TargetMemory}
\label{sec:targetmem}

The architecture API of the Memory Model is very flexible, simple and
general, but is too low-level for clients that need a model of the target
memory system of a TTA processor core. For example, the operation behaviour
models implemented by OSAL work with integer numbers of user-defined bit
width and floating-point numbers, whereas the Memory Model knows only the
bit width of its minimum addressable units.

\emph{TargetMemory} is an adaptor class that extends the memory architecture
interface (Section~\ref{ssec:mem-apiarch}). It is destined to clients that
work with target-oriented data types, such as its main clients,
\emph{Operation Set Abstraction Layer} (OSAL)~\cite{OSALDesign} and
\emph{Memory State Model} (MSM)~\cite{SimulatorDesign}.

\emph{TargetMemory} lets clients read or write data expressed as built-in
types while hiding the details of how data bits are actually laid out in the
memory model.

When constructed, \emph{TargetMemory} needs additional information that is
not present in the Memory Model (MAU order) and also information that is
present but is not retrievable (MAU bit width):

\begin{description}
\item[TargetMemory(memory : Memory, wOrder : Endianess, MAUSize : int)]%
  Create a target-friendly adaptor to the memory model \emph{memory}.
  
  The parameter \emph{wOrder} defines how the MAUs of a natural word are
  laid out in memory. \emph{Endianess} is a type that represents the
  so-called endianess, that is the sequential order of the MAUs of a natural
  word in an address range. The endianess affects the way data words are
  packed and unpacked. For example, in a memory with 4-MAU natural words and
  big endian word order, the endianess would be represented by a sequence
  such as: 1, 2, 3, 4. A little endian memory would have the following
  sequence: 4, 3, 2, 1.

\end{description}

\emph{TargetMemory} provides the following architecturally visible
interface.

\begin{description}
\item[initiateRead(address : Word, size : int, id : URC)]%
  Initiate a load request to memory. It just forwards the request to the
  memory model.

\item[initiateWrite(address : Word, size : int, data : IntWord)]%
  Initiate request to store \emph{data} integer word to memory.

\item[initiateWrite(address : Word, size : int, data : FloatWord)]%
  Initiate request to store \emph{data} single-precision floating-point
  number to memory.\note{%
    DISCUSS}
  %
  Gives an error if the bit width set for this request handler does not
  match with the minimum bit width necessary to store the number (32 bits).

\item[initiateWrite(address : Word, size : int, data : DoubleWord)]%
  Initiate request to store \emph{data} double-precision floating-point
  number to memory.\note{%
    DISCUSS}
  %
  Gives an error if the bit width set for this request handler does not
  match with the minimum bit width necessary to store the number (64 bits).

\item[readData(data : IntWord\&, id : URC)]%
  Loads a word from memory into integer word \emph{data}.

\item[readData(data : FloatWord\&, id : URC)]%
  Load a word from memory into single-precision floating-point word
  \emph{data}.

\item[readData(data : DoubleWord\&, id : URC)]%
  Load a word from memory into double-precision floating-point word
  \emph{data}.

\item[isResultReady(id : URC) : bool]%
  Return `true' if a new result is ready from the request source identified
  by \emph{id}. It just forwards the request to the memory model.
\item[advanceClock() : void]%
  Advances the clock of the memory model.
\end{description}

If the address points out of the memory range, an exception is thrown.

The restrictions of bit width apply to floating-point base data types
described in \cite{OSAL-specs}, which must match exactly or be larger by no
more than one MAU.  On the contrary, the bit width of the integer word can
be also smaller.  For example, given a MAU of 10 bits and a single precision
floating point data type of 32 bits, then 4 MAUs are necessary to store a
floating point word in memory.  A 32-bit integer, however, can be stored as
1, 2, 3 or 4 MAUs in the same memory, depending on the store operation
simulated.

\emph{TargetMemory} provides also adapted interface for architecturally
invisible memory accesses (see Section~\ref{ssec:mem-apiservice}).

\begin{description}
\item[readBlock(address : Word, data : IntWordVector\&, width : int)]%
\item[readBlock(address : Word, data : FloatWordVector\&, width : int)]%
\item[readBlock(address : Word, data : DoubleWordVector\&, width : int)]%
  Read a memory area starting at address \emph{address}, and copy it into
  vector \emph{data}.  The area size is given by the number of MAUs
  necessary to store as many words of bit width \emph{width} as given by the
  size of vector \emph{data}.

\item[writeBlock(address : Word, data : IntWordVector, width : int)]%
\item[writeBlock(address : Word, data : FloatWordVector, width : int)]%
\item[writeBlock(address : Word, data : DoubleWordVector, width : int)]%
  Copy a vector of a given built-in data type words, \emph{data} into the
  memory area starting at address \emph{address}. The area size is given by
  the number of MAUs necessary to store as many words as given by the size
  of the vector.
\end{description}

Adapting between vector of MAUs and built-in data types requires two
important activities:
\begin{enumerate}
\item Packing of MAUs.
\item Adjustment of MAU order (endianess).
\end{enumerate}

\subsection{Packing and Unpacking Words}
\label{ssec:pack}

In order to support minimum addressable units of widths that are not
commonly found in host architectures where the Memory Model is compiled, the
data stored in memory must be laid out in a consistent manner.

Because an integer word of any size allowed by the target architecture must
be also accessible MAU by MAU, it is necessary to split multi-MAU words into
single MAU and store each independently.  As a result, when a multi-MAU word
is read from memory, bits from different MAUs must be packed together.

For example, given a memory with 12-bit MAU and a ``natural'' word of 2
MAUs (24 bits) function units and register files of the target processor
will (probably) all be 24 bits. Thus, the OSAL will simulate 24-bit
operations.

In memory, a 24-bit word will be stored in the implementation of the Memory Model, in the byte-addresses memory of the host machine, using at least 2 bytes for each MAU, as shown below.%
\footnote{%
  In practice, it is likely that the implementation reserve more bytes for
  each MAU, to easily support wider MAUs.}
\begin{quote}
\begin{tabular}[h]{|c|c|}
\hline
        MAU1        &         MAU2\\
\hline
 \texttt{0000xxxx xxxxxxxx}  &  \texttt{0000yyyy yyyyyyyy}\\
\hline
\end{tabular}
\end{quote}

The implementation of the processor simulator, however, expects integer
words of the given width (in this case, 24 bits):
\begin{quote}
\begin{tabular}[h]{|c|c|}
\hline
        Natural Word\\
\hline
 \texttt{00000000 xxxxxxxx xxxxyyyy yyyyyyyy}\\
\hline
\end{tabular}
\end{quote}

\subsection{MAU Order Adjustment}

The order in which multi-MAU words are stored is a property the memory
system. The Memory Model API is completely insensitive to the MAU order.
All access requests pass through vectors of MAUs, which are read or written
in the order in which they are laid out in memory.

Unlike the Memory Model implemented by \emph{Memory} class,
\emph{TargetMemory} uses target-oriented data words encoded in the MAU order
or the host computing system. Reordering of MAUs must take place when the
endianess of target memory system and host computing systems are not equal.
This adjustment is implemented in the packing and unpacking phases (see
Sections~\ref{ssec:pack} and \ref{ssec:targmem-impl}).

\section{MemoryContents}
\label{sec:memcontents}

Internally, the minimum addressable units of a Memory are stored in a
\emph{MemoryContents} object. \emph{MemoryContents} models the data
contained in a memory. Data is stored as a vector of integers, each
representing one MAU. This restricts the maximum bit width of a MAU to the
bit width of integers on host computers, typically 32 bits. The vector index
is converted to a memory address by adding the starting address given in the
constructor of Memory to the index. The first index points to the first MAU
location of the address space, the second index to the second location, and
so on.

\begin{description}
\item[MemoryContents(start : Word, end : Word, MAUSize : int)]%
  Create an array of \emph{size}-bit MAUs.

\item[write(address : Word, data : const MAUVector\&)] %
  Store data to memory.  Write the number of MAUs given by the size of
  the vector \emph{data} into the memory area starting at given address.
  Each element of the vector contains one MAU.

  Throw \emph{OutOfRange} exception if given address is out of boundary, or
  if the vector, given the starting address, exceeds the memory upper
  boundary.

\item[read(address : Word, data : MAUVector\&, size : int)] %
  Read data from memory. Read \emph{size} MAUs from the memory area
  starting at address \emph{address}. Each MAU is stored in a separate
  element of the vector \emph{data}. If size is omitted, the number of MAUs
  read is equal to the vector size. If the original size of the vector is
  smaller than \emph{size}, the vector is resized.

  Throw \emph{OutOfRange} exception if given address is out of boundary, or
  if the vector, given the starting address, exceeds the memory upper
  boundary.

\item[read(address : Word, data : MAU*, size : int)] %
  Read data from memory. Read \emph{size} MAUs from the memory area
  starting at address \emph{address}. Each MAU is stored in a separate
  element of the vector \emph{data}.

  Throw \emph{OutOfRange} exception if given address is out of boundary, or
  if the vector, given the starting address, exceeds the memory upper
  boundary.
\end{description}


\section{Implementation}

% Describe here important low-level details. Only details about complex
% problems that require ``smart solution'' should be treated.  Do not
% describe every single detail of the implementation when that follows from
% the specification/design in a straightforward manner.

\subsection{Request Restrictions on Architectural Interface}
\label{ssec:if-restrictions}

The Memory Model does not impose special restrictions on the behaviour of
the memory system when more than one access request of the same type to the
same address is sent to memory in a single cycle.

However, for practical and reasonably efficient implementations of the
Memory Model, either of the following two cases may be considered as the
only acceptable for sure.  Other behaviours may not be guaranteed.
Applications that are affected by this aspect the Memory Model should take
into account this restriction.

\paragraph{Case 1: multiple read requests permitted.}
The memory model accepts two or more parallel read access requests to the
same address.  All requests are coalesced into a single request.  Results
returned for all requests at the same time.

\emph{Example.} Two function units that implement a load operation generate
a read access request to the same address \emph{A}.  Let us assume that the
latency of load operation is 2 cycles.  Two parallel read requests produce
the following behaviour:
\begin{verbatim}
cycle 1:
    SUa.simulTrigger(A)    // mem.initiateRead(A)
    SUb.simulTrigger(A)    // mem.initiateRead(A)
    mem.advanceClock()
cycle 2:
    SUa.lateResult()        // false
    SUb.lateResult()        // false
cycle 3: // mem.advanceClock()
    SUa.lateResult()        // true -- loadData(A)
    SUb.lateResult()        // true -- loadData(A) -- same result
\end{verbatim}

Implementation-dependent, hard-to-predict aspects of the Memory Model such
as this are a strong case in favour of tracking memory accesses
\emph{inside} the Memory Model implementation. See~\cite{ToolkitDesign}
\emph{TrackedMemory} for details.

\paragraph{Case 2: strictly sequential access.}
The memory model does not accept more than one parallel read access request
to the same address.  The first request is accepted, the following are
denied until the first request completes, and result in the a global lock
request of the target processor.

\emph{Example.} Two function units that implement a load operation generate
a read access request to the same address \emph{A}.  Let us assume that the
latency of load operation is 2 cycles.  Two parallel read requests produce
the following behaviour:
\begin{verbatim}
cycle 1:
    SUa.simulTrigger(A)     // mem.initiateRead(A)
    SUb.simulTrigger(A)     // mem.initiateRead(A)
    mem.advanceClock()
cycle 2:
    // lock processor, mem.isAvailable() == false
    SUa.lateResult()        // false
    SUa.lateResult()        // false
    mem.advanceClock()
cycle 3:
    // unlock processor, mem.isAvailable() == true
    SUa.lateResult()        // true -- loadData(A)
    SUb.lateResult()        // false
    mem.advanceClock()
cycle 4:
    SUb.lateResult()        // false
    mem.advanceClock()
cycle 5:
    SUb.lateResult()        // true -- loadData(A)
\end{verbatim}
%
At the end of cycle 1, one of the two requests (it does not matter which, in
principle), is stalled, The other request is accepted and can proceed until
it is completed, in cycle 3. In this cycle, the request that was blocked is
accepted and its result is delivered after two cycles, as if it were
initiated in cycle 3.

\subsection{Identification of Memory Access Requests}
\label{ssec:request-id}

Access requests to memory are uniquely identified by three items of
information:
\begin{enumerate}
\item The address of the access.
\item The cycle in which the request is made.
\item The source of the request.
\end{enumerate}

The Memory Model must satisfy an important restriction: late-coming results
from the same source of requests arrive in chronological order.  Thanks to
this restriction, request cycle and address of the access are not necessary
to identify the memory access request in the internal state of the Memory
Model.

Requests don't need to be identified by address, because any given source of
requests can receive the result of one pending request only at any given
cycle.  Later requests from the same source are simply kept pending until
the older requests are satisfied.

A client of the Memory Model (that is, a source of access requests) does not
need to check in every cycle for late-coming results of pending requests.
Such results can be simply ignored if the client should not need them
anymore (a common situation when an instruction scheduler performs
speculation).  However, it is responsibility of the client to bookkeep the
arrival of late-coming results \emph{if more results are pending}, to ensure
that a late result is associated to the request that originated it.

\emph{Example.} Two sources of requests (function units) initiate
independent load requests in different cycles to the same address \emph{A}:
\begin{verbatim}
cycle 1:
    SUa : mem.initiateRead(A)
cycle 2:
    SUa : mem.loadData(A,data) // check for request in cycle 1
    SUb : mem.initiateRead(A)
\end{verbatim}
%
after both requests have been performed, the client of the Memory Model, if
it is simulating a processor, will test any incoming result from any of the
pending read access requests:
\begin{verbatim}
cycle 3:
    SUa : mem.loadData(A,data) // check for request in cycle 1
    SUb : mem.loadData(A,data) // check for request in cycle 2
cycle 4:
    SUa : mem.loadData(A,data) // check for request in cycle 1
    SUb : mem.loadData(A,data) // check for request in cycle 2
\end{verbatim}
%
The results of multiple load accesses to the same address from different
units are not necessarily satisfied in the same order in which the requests
were made.

\emph{Example.} One function unit initiates two requests in different cycles
to the same address \emph{A}:
\begin{verbatim}
cycle 1:
    SUa : mem.initiateRead(A)
cycle 2:
    SUa : mem.initiateRead(A)
\end{verbatim}
%
In this case, the older request will be satisfied first.
\begin{verbatim}
cycle 3:
    SUa : mem.loadData(A,data) // check for request in cycle 1
cycle 4:
    SUa : mem.loadData(A,data) // check for request in cycle 1
\end{verbatim}
%
The client should test for late-coming results only for the older request
until it is satisfied.  If the request becomes irrelevant (for example, due
to misspeculation), the client should still check for the late result until
it arrives, in order to know when it should start to check for the result of
the newer request.


\subsection{Ordering Constraints}
\label{ssec:order-impl}

The constraints on the order in which load requests from memory are
satisfied implies some type of FIFO data structure for pending load
requests.

\subsection{TargetMemory}
\label{ssec:targmem-impl}

To assist in unpacking and packing of words of target-oriented data types to
and from requested amount of MAUs, \emph{TargetMemory} provides the
following private methods:
\begin{description}
\item[pack(data : MAUVector, size : int, value : IntWord\&)]%
  Pack \emph{size} MAUs to a single \emph{IntWord}.

\item[pack(data : MAUVector, size : int, value : FloatWord\&)]%
  Pack \emph{size} MAUs to a single \emph{FloatWord}.

\item[pack(data : MAUVector, size : int, value : DoubleWord\&)]%
  Pack \emph{size} MAUs to a single \emph{DoubleWord}.

\item[unpack(value : const IntWord\&, size : int, MAUVector\& : data]%
  Unpack a given \emph{IntWord} to \emph{size} MAUs, and store the result
  in vector \emph{data}.
\end{description}

Note that most of these methods are very light-weight; for example,
(un)packing FloatWord numbers is just a reinterpretation type cast of the
same operation on IntWord numbers.

\section{Error Handling}
\label{sec:memory-errors}

% How are errors handled? What is considered internal and what is considered
% external source of errors? What is handled by throwing exceptions and what
% by assertions? What are the 

\subsection{Misalignment}

The three typical behaviours of the memory model in case of a word that is
not aligned to an address corresponding to a multiple of its size is
requested are:
\begin{enumerate}
\item Abort the program (throw exception)
\item Ignore the misalignment (zero-out lower bits)
\item Handle the operation (no misalignment at all)
\end{enumerate}

An implementation of the memory model is free to choose any of the three
methods. During earlier stages of architecture evaluation and application
development, aborting is probably more useful.

The implementation provided with the toolset distribution will abort the
program.

% ADD NEW SECTIONS ON DESIGN HERE



\chapter{REJECTED ALTERNATIVES}

Rejected alternatives for parts of the design are described here in
chronological order with the reasons why the alternative was rejected.  For
future reference.

\begin{description}
\item[30.03.2004 --- Interface of Request Initiation Methods] %
  The methods that initiate a memory access could return a Boolean flag to
  tell whether memory has accepted the request (it was ready) or the request
  has been ignored.  The return value is thus identical to the return value
  of the functions that test for memory readiness for a specific access
  request.

  The alternative whereby the request methods return nothing and the client
  ought to call the methods that test for readiness as been chosen because
  of the following advantages:
  \begin{enumerate}
  \item %
    The methods do only one thing: try to initiate a request.  Always better
    to have methods that do one thing.
  \item %
    Avoid double testing (redundant) or ignoring the return value/asserting
    it (extra work).
  \end{enumerate}

\end{description}



\chapter{IDEAS FOR FURTHER DEVELOPMENT}

% Ideas that are not part of the design yet but that might be added in the
% future are listed here. This is to have ideas written down somewhere.
% This list should contain the date of addition, the idea described briefly
% and the inventor of the idea.

% Example:
%
% 18.11.2003 . . . -P.J‰‰skel‰inen
\begin{description}
\item[31.08.2004 --- Public Methods to Lock Memory.]%
  It may be possible in the future that the memory lock condition depend on
  activities that cannot be modelled as memory accesses. In that case, a
  public interface to let clients force locking becomes necessary:
  \begin{description}
  \item[lock()]%
    Locks memory.  A locked memory does not accept new access requests.
    An access request will make the memory unavailable.
  \item[unlock()]%
    Unlocks memory.  After it is unlocked, the memory module can accept an
    access request, if no other condition (for example, concurrent access
    compete for the same memory port) applies.
  \end{description}
  %
  The lock state should be implemented with a software semaphore.  Lock
  requests may be sent even when the memory is already locked.  The Memory
  Model remembers the number of lock requests it received and doesn't unlock
  itself until as many unlock requests have been received.
  
  Possible error conditions occur when a client sends an unlock request to a
  memory that is not locked. Spurious unlocks are a potential source of
  errors that are extremely difficult to detect.  Errors occur if a spurious
  unlock request is received when the processor is in a locked state caused
  by an unrelated lock request.
  -- A.Cilio
\end{description}



%\chapter{PENDING ISSUES}

% Pending issues concerning this design, a sort of TODO list.
% This chapter should be empty when implementation of this module/subsystem
% is completed.



\chapter{MAINTENANCE}

% This chapter treats the problem of extending the design with new
% capabilities that fit in a predefined framework.

% For example, in case of the TPEF module, this chapter could describe the
% steps that must be taken in order to add a new Section subclass and/or a
% SectionReader subclass.

% Remove this chapter no obvious and standardised way to extend the design
% with new capabilities exists.


% ------------------------------------------------------------------------

%% Remove this part if there are no references.  Usually there will be at
%% least a reference to the functional specifications of the same module.

%  References are generated with BibTeX from a bibtex file.
\bibliographystyle{alpha}
\cleardoublepage
%% Equivalent to a chapter in the table of contents
\addcontentsline{toc}{chapter}{BIBLIOGRAPHY}
\bibliography{Bibliography}



\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
